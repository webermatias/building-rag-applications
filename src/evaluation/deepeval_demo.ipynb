{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "424b4361-f20c-4f54-afff-7ba76578667c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "envs = load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7dba2cec-e732-4a67-9e59-bfaf14d2e1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import FaithfulnessMetric, AnswerRelevancyMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.dataset import EvaluationDataset\n",
    "\n",
    "from src.database.database_utils import get_weaviate_client\n",
    "from src.database.weaviate_interface_v4 import WeaviateWCS\n",
    "from src.llm.llm_interface import LLM\n",
    "from src.llm.llm_utils import get_token_count, load_azure_openai\n",
    "from src.llm.prompt_templates import question_answering_prompt_series, huberman_system_message\n",
    "from app_features import generate_prompt_series\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "from litellm import ModelResponse\n",
    "\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "from rich import print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f89c8ad-7fb8-4982-accd-dba8436fc84e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.7, model='gpt-4', strict_mode=True)\n",
    "# test_case = LLMTestCase(\n",
    "#     input=\"What if these shoes don't fit?\",\n",
    "#     # Replace this with the actual output from your LLM application\n",
    "#     actual_output=\"We offer a 30-day full refund at no extra costs.\",\n",
    "#     retrieval_context=[\"All customers are eligible for a 30 day full refund at no extra costs.\"]\n",
    "# )\n",
    "# evaluate([test_case], [answer_relevancy_metric], run_async=False, ignore_errors=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2d4ebecf-5553-4707-8ee8-1e35fc1fc676",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\"Give a brief explanation of how brain neuroplasticity works\",\n",
    "             \"What is the role of dopamine in the body\",\n",
    "             \"What is a catecholimine\",\n",
    "             \"What does Jocko Willink have to say about leadership\",\n",
    "             \"What does Lex Fridman think about the evolution of AI\", \n",
    "             \"How can I support the Huberman Lab podcst\",\n",
    "             \"Why do people make self-destructive decisions\",\n",
    "             \"Provide a better sleep protocol in list format\",\n",
    "             \"What are the topcis that Lex Fridman discusses\",\n",
    "             \"Is there a generally positive outlook on the future of AI\",\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "444bf30f-9a5b-4994-8a77-3c0ef571e40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2247250/3431604043.py:1: ResourceWarning: unclosed <ssl.SSLSocket fd=68, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('10.18.0.6', 32790), raddr=('34.149.137.116', 443)>\n",
      "  client = get_weaviate_client()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "client = get_weaviate_client()\n",
    "turbo = LLM(model_name='gpt-3.5-turbo-0125')\n",
    "azure = load_azure_openai(model_name='gpt-4')\n",
    "collection_name = 'Huberman_minilm_128'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c7aca859-f202-48e9-b329-e39ca6505db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_bundle(query: str,\n",
    "                      client: WeaviateWCS,\n",
    "                      collection_name: str,\n",
    "                      answer_llm: LLM,\n",
    "                      ground_truth_llm: LLM=None\n",
    "                     ) -> tuple[str, list[list[str]], str]:\n",
    "    '''\n",
    "    Returns answer, ground truth and associated context from a single query.\n",
    "    '''\n",
    "    def format_llm_response(response: ModelResponse) -> str:\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    #1st-stage retrieval (get contexts)\n",
    "    context = client.hybrid_search(query, collection_name, \n",
    "                                   query_properties=['content', 'title', 'summary'],\n",
    "                                   limit=3, \n",
    "                                   return_properties=['content', 'guest', 'summary'])\n",
    "    #create contexts from content field\n",
    "    contexts = [d['content'] for d in context]\n",
    "    \n",
    "    #generate assistant message prompt\n",
    "    assist_message = generate_prompt_series(query, context)\n",
    "\n",
    "    #generate answers from model being evaluated\n",
    "    answer = format_llm_response(answer_llm.chat_completion(huberman_system_message, assist_message))\n",
    "\n",
    "    #create ground truth answers\n",
    "    if ground_truth_llm:\n",
    "        ground_truth = format_llm_response(ground_truth_llm.chat_completion(huberman_system_message, assist_message))\n",
    "        return query, contexts, answer, ground_truth\n",
    "    return query, contexts, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a6a84919-1f2d-43e9-9527-aa3b81ae07ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "from time import sleep\n",
    "\n",
    "async def create_test_dataset(questions: list[str], \n",
    "                              client: WeaviateWCS,\n",
    "                              collection_name: str,\n",
    "                              answer_llm: LLM,\n",
    "                              ground_truth_llm: LLM=None, \n",
    "                              batch_size: int=5, \n",
    "                              async_mode: bool=True,\n",
    "                              disable_internal_tqdm: bool=False):\n",
    "    total = len(questions)\n",
    "    progress = tqdm('Queries', total=total, disable=disable_internal_tqdm)\n",
    "    data = []\n",
    "    batches = ceil(total/batch_size)\n",
    "    for i in range(batches):\n",
    "        batch = questions[i*batch_size:(i+1)*batch_size]\n",
    "        if async_mode:\n",
    "            results = await asyncio.gather(*[aget_answer_bundle(query, \n",
    "                                                                client, \n",
    "                                                                collection_name, \n",
    "                                                                answer_llm,\n",
    "                                                                ground_truth_llm) for query in batch])\n",
    "            if any(results):\n",
    "                data.extend(results)\n",
    "            else:\n",
    "                raise \"No results returned for initial batch, double-check your inputs.\"\n",
    "        else:\n",
    "            with ThreadPoolExecutor(max_workers=os.cpu_count() * 2) as executor:\n",
    "                futures = [executor.submit(get_answer_bundle, query, client, collection_name, answer_llm, ground_truth_llm) for query in batch]\n",
    "                for future in as_completed(futures):\n",
    "                    progress.update(1)\n",
    "                    data.append(future.result())\n",
    "        print(f\"Finished with batch {i+1}, taking a break...\")\n",
    "    \n",
    "    queries = [d[0] for d in data]\n",
    "    contexts = [d[1] for d in data]\n",
    "    answers = [d[2] for d in data]\n",
    "    dataset = {'queries': queries, 'contexts': contexts, 'answers': answers}\n",
    "    if len(data[0]) == 4:\n",
    "        ground_truths = [d[3] for d in data]\n",
    "        dataset.update(ground_truths=ground_truths)\n",
    "        return dataset\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e48edd30-c6b9-40be-80cd-cf43c25f2c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def aget_answer_bundle( query: str,\n",
    "                              client: WeaviateWCS,\n",
    "                              collection_name: str,\n",
    "                              answer_llm: LLM,\n",
    "                              ground_truth_llm: LLM=None\n",
    "                             ) -> tuple[str, list[list[str]], str]:\n",
    "    '''\n",
    "    Returns answer, ground truth and associated context from a single query.\n",
    "    '''\n",
    "    #1st-stage retrieval (get contexts)\n",
    "    context = client.hybrid_search(query, collection_name, \n",
    "                                   query_properties=['content', 'title', 'summary'],\n",
    "                                   limit=3, \n",
    "                                   return_properties=['content', 'guest', 'summary'])\n",
    "    \n",
    "    #create contexts from content field\n",
    "    contexts = [d['content'] for d in context]\n",
    "    \n",
    "    #generate assistant message prompt\n",
    "    assist_message = generate_prompt_series(query, context, 2)\n",
    "\n",
    "    #generate answers from model being evaluated\n",
    "    answer = await answer_llm.achat_completion(huberman_system_message, assist_message)\n",
    "\n",
    "    #create ground truth answers\n",
    "    if ground_truth_llm:\n",
    "        ground_truth = await ground_truth_llm.achat_completion(huberman_system_message, assist_message)\n",
    "        return query, contexts, answer, ground_truth\n",
    "    return query, contexts, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "48c7f866-1e79-46b8-9738-2ae9353eb2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                                                                                                          | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A/anaconda/envs/vsa/lib/python3.10/site-packages/openai/_legacy_response.py:347: ResourceWarning: unclosed <socket.socket fd=131, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('10.18.0.6', 56944), raddr=('52.242.46.17', 443)>\n",
      "  async def wrapped(*args: P.args, **kwargs: P.kwargs) -> LegacyAPIResponse[R]:\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/anaconda/envs/vsa/lib/python3.10/site-packages/openai/_legacy_response.py:347: ResourceWarning: unclosed <socket.socket fd=136, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('10.18.0.6', 44686), raddr=('104.18.7.192', 443)>\n",
      "  async def wrapped(*args: P.args, **kwargs: P.kwargs) -> LegacyAPIResponse[R]:\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/anaconda/envs/vsa/lib/python3.10/asyncio/selector_events.py:704: ResourceWarning: unclosed transport <_SelectorSocketTransport fd=136 read=idle write=<idle, bufsize=0>>\n",
      "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Finished with batch <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, taking a break<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Finished with batch \u001b[1;36m1\u001b[0m, taking a break\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Finished with batch <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, taking a break<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Finished with batch \u001b[1;36m2\u001b[0m, taking a break\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                          | 0/10 [00:37<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "data = await create_test_dataset(questions, client, collection_name, turbo, azure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "766a8a26-c10b-4d13-af2e-9379409eadc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_eval_dataset(questions: list[str],\n",
    "                        contexts: list[list[str]],\n",
    "                        answers: list[str]\n",
    "                       ) -> EvaluationDataset:\n",
    "    assert len(questions) == len(contexts) == len(answers), 'Mismatched lengths in input values, retry after correcting'\n",
    "    test_cases = []\n",
    "    for i in range(len(questions)):\n",
    "        test_case = LLMTestCase(input=questions[i],\n",
    "                                actual_output=answers[i],\n",
    "                                retrieval_context=contexts[i])\n",
    "        test_cases.append(test_case)\n",
    "    return EvaluationDataset(alias='Initial test', test_cases=test_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0ce481-bae0-4d01-ae76-f57b36434c9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vsa",
   "language": "python",
   "name": "vsa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
